{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "\n",
    "from scipy import sparse as sp\n",
    "from scipy import sparse\n",
    "from scipy.sparse import spdiags\n",
    "from scipy.sparse import linalg\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.io import savemat,loadmat\n",
    "import scipy.integrate as integrate\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation \n",
    "\n",
    "from itertools import combinations_with_replacement, product\n",
    "from sklearn.decomposition import SparseCoder\n",
    "from tqdm.notebook import tqdm,trange\n",
    "import sys,time\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import pysindy as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 19 11:39:32 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.95.01    Driver Version: 440.95.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    67W / 300W |  13414MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    52W / 300W |   9190MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    50W / 300W |   6664MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    54W / 300W |   6664MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      1372      C   python                                      6653MiB |\n",
      "|    0      1373      C   python                                       861MiB |\n",
      "|    0      1374      C   python                                       861MiB |\n",
      "|    0      1375      C   python                                       861MiB |\n",
      "|    0     75448      C   /g/g92/he10/.conda/envs/tfvenv/bin/python    997MiB |\n",
      "|    0     77680      C   .../fries4/GitHub/NM-ROM/nm-rom/bin/python  1467MiB |\n",
      "|    0    127909      C   .../fries4/GitHub/NM-ROM/nm-rom/bin/python  1697MiB |\n",
      "|    1      1373      C   python                                      6653MiB |\n",
      "|    1     80180      C   /g/g92/he10/.conda/envs/tfvenv/bin/python    997MiB |\n",
      "|    1    113716      C   /g/g92/he10/.conda/envs/tfvenv/bin/python   1525MiB |\n",
      "|    2      1374      C   python                                      6653MiB |\n",
      "|    3      1375      C   python                                      6653MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Auto-Encoder\n",
    "We now train the auto-encoder as per the \"train_NM-ROM_swish\" file. We use the snapshot data from above to train the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose device that is not being used\n",
    "gpu_ids = \"0\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=gpu_ids\n",
    "\n",
    "def create_mask_v2(m,b,db):\n",
    "    '''\n",
    "    mask=create_mask_v2(m,b,db)  \n",
    "    '''\n",
    "    \n",
    "    M2 = b + db*(m-1)\n",
    "    mask = np.zeros((m,M2),dtype='int')\n",
    "    \n",
    "    block = np.ones(b,dtype='int')\n",
    "    ind = np.arange(b)\n",
    "    for row in range(m):\n",
    "        col = ind + row*db\n",
    "        mask[row,col] = block\n",
    "    \n",
    "    print(\n",
    "        \"Sparsity in {} by {} mask: {:.2f}%\".format(\n",
    "            m, M2, (1.0-np.count_nonzero(mask)/mask.size)*100\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.spy(mask)\n",
    "    plt.show()\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def silu(input):\n",
    "    return input * torch.sigmoid(input)\n",
    "class SiLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return silu(input)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,m,M1,f):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.full = nn.Sequential(\n",
    "            nn.Linear(m,M1),\n",
    "            SiLU(),\n",
    "            nn.Linear(M1,f,bias=False)\n",
    "        )\n",
    "                \n",
    "    def forward(self, y):     \n",
    "        y = y.view(-1,m)\n",
    "        T = self.full(y)\n",
    "        T = T.squeeze()\n",
    "        \n",
    "        return T\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,f,M2,m):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.full = nn.Sequential(\n",
    "            nn.Linear(f,M2,bias=False),\n",
    "            SiLU(),\n",
    "            nn.Linear(M2,m,bias=False)\n",
    "        )\n",
    "             \n",
    "    def forward(self,T):\n",
    "        T = T.view(-1,f)\n",
    "        y = self.full(T)\n",
    "        y = y.squeeze()\n",
    "        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3604, 1000]) torch.Size([400, 1000])\n",
      "(3604, 1000) (400, 1000)\n",
      "Using device: cuda \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nx = 1001\n",
    "dx = 6 / (nx - 1)\n",
    "nt = 1000\n",
    "tstop = 1\n",
    "x=np.linspace(-3, 3, nx)\n",
    "\n",
    "dt = tstop / nt \n",
    "c = dt/dx\n",
    "t = np.linspace(0, tstop, nt)\n",
    "\n",
    "# load solution snapshot\n",
    "solution_snapshot_orig = pickle.load(open(\"./data/snapshot_git.p\", 'rb'))\n",
    "\n",
    "# substract IC -> centered on IC\n",
    "ndata=solution_snapshot_orig.shape[0]\n",
    "nset= round(ndata/(nt+1))\n",
    "\n",
    "solution_snapshot=np.array([])\n",
    "for foo in range(nset):\n",
    "    solution_snapshot=np.append(solution_snapshot,solution_snapshot_orig[foo*(nt+1):(foo+1)*(nt+1)])#    -solution_snapshot_orig[foo*(nt+1)])\n",
    "\n",
    "solution_snapshot=np.reshape(solution_snapshot,(-1,nx))\n",
    "\n",
    "# remove BC\n",
    "solution_snapshot = solution_snapshot[:,:-1].astype('float32')\n",
    "\n",
    "# define testset and trainset indices\n",
    "test_ind = np.random.permutation(np.arange(solution_snapshot.shape[0]))[:int(0.1*solution_snapshot.shape[0])]\n",
    "train_ind = np.setdiff1d(np.arange(solution_snapshot.shape[0]),test_ind)\n",
    "\n",
    "# set trainset and testset\n",
    "trainset = solution_snapshot[train_ind]\n",
    "testset = solution_snapshot[test_ind] \n",
    "\n",
    "# set dataset\n",
    "dataset = {'train':data_utils.TensorDataset(torch.tensor(trainset)),\n",
    "           'test':data_utils.TensorDataset(torch.tensor(testset))}\n",
    "print(dataset['train'].tensors[0].shape, dataset['test'].tensors[0].shape)\n",
    "\n",
    "\n",
    "# compute dataset shapes\n",
    "dataset_shapes = {'train':trainset.shape,\n",
    "                 'test':testset.shape}\n",
    "\n",
    "print(dataset_shapes['train'],dataset_shapes['test'])\n",
    "\n",
    "\n",
    "# set batch_size, number of epochs, paitience for early stop\n",
    "batch_size = 20\n",
    "num_epochs = 1000\n",
    "num_epochs_print = num_epochs//100\n",
    "early_stop_patience = num_epochs//10\n",
    "\n",
    "\n",
    "# set data loaders\n",
    "train_loader = DataLoader(dataset=dataset['train'], \n",
    "                          batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(dataset=dataset['test'], \n",
    "                         batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "data_loaders = {'train':train_loader, 'test':test_loader}\n",
    "\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device, '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in 1000 by 12024 mask: 99.70%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------checkpoint restored--------\n",
      "\n",
      "Sparsity in 1000 by 12024 mask: 99.70%\n",
      "\n",
      "Re-start 21th training... m=1000, f=4, M1=2000, M2=12024, b=36, db=12\n",
      "\n",
      "Epoch 30/1000, Learning rate 0.001\n",
      "----------\n",
      "train MSELoss: 1.3861688449557028e-05\n",
      "test MSELoss: 1.104671792973022e-06\n",
      "\n",
      "Epoch 40/1000, Learning rate 0.001\n",
      "----------\n",
      "train MSELoss: 2.249150886728299e-06\n",
      "test MSELoss: 8.511886278483871e-07\n",
      "\n",
      "Epoch 50/1000, Learning rate 0.001\n",
      "----------\n",
      "train MSELoss: 2.185314889474987e-05\n",
      "test MSELoss: 2.937123899755534e-05\n",
      "\n",
      "Epoch 60/1000, Learning rate 0.0001\n",
      "----------\n",
      "train MSELoss: 1.7632032667555496e-07\n",
      "test MSELoss: 1.810226990528463e-07\n",
      "\n",
      "Epoch 70/1000, Learning rate 0.0001\n",
      "----------\n",
      "train MSELoss: 1.649990788468389e-07\n",
      "test MSELoss: 2.1596123644940235e-07\n",
      "\n",
      "Epoch 80/1000, Learning rate 0.0001\n",
      "----------\n",
      "train MSELoss: 2.2481041368253642e-07\n",
      "test MSELoss: 2.0098768089837903e-07\n",
      "\n",
      "Epoch 90/1000, Learning rate 1e-05\n",
      "----------\n",
      "train MSELoss: 1.0231477188432457e-07\n",
      "test MSELoss: 1.0564279904201612e-07\n",
      "\n",
      "Epoch 100/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 9.326809492885504e-08\n",
      "test MSELoss: 9.867477039904315e-08\n",
      "\n",
      "Epoch 110/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 9.277548047648059e-08\n",
      "test MSELoss: 9.90842462300634e-08\n",
      "\n",
      "Epoch 120/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 9.246385755959051e-08\n",
      "test MSELoss: 9.735979986658094e-08\n",
      "\n",
      "Epoch 130/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 9.19123148644891e-08\n",
      "test MSELoss: 9.82631842560977e-08\n",
      "\n",
      "Epoch 140/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 9.091446153560492e-08\n",
      "test MSELoss: 9.738537869452557e-08\n",
      "\n",
      "Epoch 150/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 9.063526761962257e-08\n",
      "test MSELoss: 9.669332712292089e-08\n",
      "\n",
      "Epoch 160/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 9.047749743696006e-08\n",
      "test MSELoss: 9.617763510050282e-08\n",
      "\n",
      "Epoch 170/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.956754829933945e-08\n",
      "test MSELoss: 9.496593484925598e-08\n",
      "\n",
      "Epoch 180/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.940658449520903e-08\n",
      "test MSELoss: 9.39391334497941e-08\n",
      "\n",
      "Epoch 190/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.846684998968095e-08\n",
      "test MSELoss: 9.39464069205087e-08\n",
      "\n",
      "Epoch 200/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.809937253068709e-08\n",
      "test MSELoss: 9.312395974347964e-08\n",
      "\n",
      "Epoch 210/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.769931292950488e-08\n",
      "test MSELoss: 9.372922811223816e-08\n",
      "\n",
      "Epoch 220/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.676445531408819e-08\n",
      "test MSELoss: 9.249283561274524e-08\n",
      "\n",
      "Epoch 230/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.649068862367577e-08\n",
      "test MSELoss: 9.15554569047572e-08\n",
      "\n",
      "Epoch 240/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.592382978165412e-08\n",
      "test MSELoss: 9.093272907989558e-08\n",
      "\n",
      "Epoch 250/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.561831120759377e-08\n",
      "test MSELoss: 9.015335571405103e-08\n",
      "\n",
      "Epoch 260/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.501934884213156e-08\n",
      "test MSELoss: 9.051388758507528e-08\n",
      "\n",
      "Epoch 270/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.440338318900261e-08\n",
      "test MSELoss: 8.906941566522164e-08\n",
      "\n",
      "Epoch 280/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.374568713230392e-08\n",
      "test MSELoss: 8.858329145766674e-08\n",
      "\n",
      "Epoch 290/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.32196332375687e-08\n",
      "test MSELoss: 8.888500673265298e-08\n",
      "\n",
      "Epoch 300/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.299916401866089e-08\n",
      "test MSELoss: 8.735115386571124e-08\n",
      "\n",
      "Epoch 310/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.266715441227063e-08\n",
      "test MSELoss: 8.692438058233165e-08\n",
      "\n",
      "Epoch 320/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.19646158221365e-08\n",
      "test MSELoss: 8.653925931412232e-08\n",
      "\n",
      "Epoch 330/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.168310893971227e-08\n",
      "test MSELoss: 8.628718681791269e-08\n",
      "\n",
      "Epoch 340/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.085001458064474e-08\n",
      "test MSELoss: 8.558363262523017e-08\n",
      "\n",
      "Epoch 350/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.075056052115448e-08\n",
      "test MSELoss: 8.588036841672419e-08\n",
      "\n",
      "Epoch 360/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.010673409304853e-08\n",
      "test MSELoss: 8.588271143139536e-08\n",
      "\n",
      "Epoch 370/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 8.009234262562516e-08\n",
      "test MSELoss: 8.533826179046855e-08\n",
      "\n",
      "Epoch 380/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.975131457582515e-08\n",
      "test MSELoss: 8.618113795932914e-08\n",
      "\n",
      "Epoch 390/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.910865781067984e-08\n",
      "test MSELoss: 8.434513194544025e-08\n",
      "\n",
      "Epoch 400/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.825330596460087e-08\n",
      "test MSELoss: 8.337765908805749e-08\n",
      "\n",
      "Epoch 410/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.793449995748015e-08\n",
      "test MSELoss: 8.23982865938433e-08\n",
      "\n",
      "Epoch 420/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.792120363277789e-08\n",
      "test MSELoss: 8.212612758740079e-08\n",
      "\n",
      "Epoch 430/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.773811965539982e-08\n",
      "test MSELoss: 8.362929193594937e-08\n",
      "\n",
      "Epoch 440/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.691932541051423e-08\n",
      "test MSELoss: 8.127509207866978e-08\n",
      "\n",
      "Epoch 450/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.63747195761669e-08\n",
      "test MSELoss: 8.045584749538648e-08\n",
      "\n",
      "Epoch 460/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.604986991413681e-08\n",
      "test MSELoss: 8.189369822275694e-08\n",
      "\n",
      "Epoch 470/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.555936319097118e-08\n",
      "test MSELoss: 8.010749041886811e-08\n",
      "\n",
      "Epoch 480/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.525317431329058e-08\n",
      "test MSELoss: 8.208237360918247e-08\n",
      "\n",
      "Epoch 490/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.515837473062895e-08\n",
      "test MSELoss: 7.930587244686649e-08\n",
      "\n",
      "Epoch 500/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.474998787220066e-08\n",
      "test MSELoss: 7.877917198584328e-08\n",
      "\n",
      "Epoch 510/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.414402353393001e-08\n",
      "test MSELoss: 7.952231300123458e-08\n",
      "\n",
      "Epoch 520/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.43673851507141e-08\n",
      "test MSELoss: 7.763973535190871e-08\n",
      "\n",
      "Epoch 530/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.329904258745198e-08\n",
      "test MSELoss: 7.752164954411e-08\n",
      "\n",
      "Epoch 540/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.357879315570454e-08\n",
      "test MSELoss: 7.88570606147232e-08\n",
      "\n",
      "Epoch 550/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.281362746684827e-08\n",
      "test MSELoss: 7.630114655654552e-08\n",
      "\n",
      "Epoch 560/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.249733848047895e-08\n",
      "test MSELoss: 7.646556561269336e-08\n",
      "\n",
      "Epoch 570/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.187790622261828e-08\n",
      "test MSELoss: 7.611288896924861e-08\n",
      "\n",
      "Epoch 580/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.178450549040882e-08\n",
      "test MSELoss: 7.549822083063873e-08\n",
      "\n",
      "Epoch 590/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.124961144249881e-08\n",
      "test MSELoss: 7.658826657319651e-08\n",
      "\n",
      "Epoch 600/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.097312477320128e-08\n",
      "test MSELoss: 7.491519049551698e-08\n",
      "\n",
      "Epoch 610/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.085791315887428e-08\n",
      "test MSELoss: 7.454756971725373e-08\n",
      "\n",
      "Epoch 620/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 7.073429773637628e-08\n",
      "test MSELoss: 7.456647033166064e-08\n",
      "\n",
      "Epoch 630/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.997257909852573e-08\n",
      "test MSELoss: 7.476637122749708e-08\n",
      "\n",
      "Epoch 640/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.964714750514952e-08\n",
      "test MSELoss: 7.349756288022036e-08\n",
      "\n",
      "Epoch 650/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.945209536595407e-08\n",
      "test MSELoss: 7.296992965422078e-08\n",
      "\n",
      "Epoch 660/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.884119932878164e-08\n",
      "test MSELoss: 7.236404737653857e-08\n",
      "\n",
      "Epoch 670/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.859625936110208e-08\n",
      "test MSELoss: 7.314742944686259e-08\n",
      "\n",
      "Epoch 680/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.82263782290898e-08\n",
      "test MSELoss: 7.172691613988036e-08\n",
      "\n",
      "Epoch 690/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.815957069042967e-08\n",
      "test MSELoss: 7.502910204237878e-08\n",
      "\n",
      "Epoch 700/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.776262719373621e-08\n",
      "test MSELoss: 7.168340072638557e-08\n",
      "\n",
      "Epoch 710/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.726984833864908e-08\n",
      "test MSELoss: 7.173437381879921e-08\n",
      "\n",
      "Epoch 720/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.698294786905713e-08\n",
      "test MSELoss: 7.111829383177338e-08\n",
      "\n",
      "Epoch 730/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.682262552098504e-08\n",
      "test MSELoss: 7.109666153581883e-08\n",
      "\n",
      "Epoch 740/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.669508938518315e-08\n",
      "test MSELoss: 7.112011335408397e-08\n",
      "\n",
      "Epoch 750/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.618189620355898e-08\n",
      "test MSELoss: 7.00270357256727e-08\n",
      "\n",
      "Epoch 760/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.610005136065701e-08\n",
      "test MSELoss: 7.027938639936337e-08\n",
      "\n",
      "Epoch 770/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.544502710236791e-08\n",
      "test MSELoss: 7.015416016997734e-08\n",
      "\n",
      "Epoch 780/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.526792730644702e-08\n",
      "test MSELoss: 6.978981801353256e-08\n",
      "\n",
      "Epoch 790/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.488511292050952e-08\n",
      "test MSELoss: 6.886214691803616e-08\n",
      "\n",
      "Epoch 800/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.471127226424797e-08\n",
      "test MSELoss: 6.914002561586586e-08\n",
      "\n",
      "Epoch 810/1000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 6.46338057486188e-08\n",
      "test MSELoss: 6.86431233631879e-08\n",
      "\n",
      "Epoch 820/1000, Learning rate 1.0000000000000002e-07\n",
      "----------\n",
      "train MSELoss: 6.315325764472233e-08\n",
      "test MSELoss: 6.727150765328816e-08\n",
      "\n",
      "Epoch 830/1000, Learning rate 1.0000000000000002e-07\n",
      "----------\n",
      "train MSELoss: 6.312423096059537e-08\n",
      "test MSELoss: 6.722342309473106e-08\n",
      "\n",
      "Epoch 840/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.29952840859598e-08\n",
      "test MSELoss: 6.717785190346604e-08\n",
      "\n",
      "Epoch 850/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.299122677653241e-08\n",
      "test MSELoss: 6.71495556048285e-08\n",
      "\n",
      "Epoch 860/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.299029461701574e-08\n",
      "test MSELoss: 6.716976947984677e-08\n",
      "\n",
      "Epoch 870/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.29810458268494e-08\n",
      "test MSELoss: 6.71655758566203e-08\n",
      "\n",
      "Epoch 880/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.298427368606745e-08\n",
      "test MSELoss: 6.716833187425663e-08\n",
      "\n",
      "Epoch 890/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.298570889564596e-08\n",
      "test MSELoss: 6.71614657221653e-08\n",
      "\n",
      "Epoch 900/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.297788320586423e-08\n",
      "test MSELoss: 6.713307119099454e-08\n",
      "\n",
      "Epoch 910/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.29762844610503e-08\n",
      "test MSELoss: 6.715274860624732e-08\n",
      "\n",
      "Epoch 920/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.296935785230017e-08\n",
      "test MSELoss: 6.717355578444995e-08\n",
      "\n",
      "Epoch 930/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.298098117613522e-08\n",
      "test MSELoss: 6.713474363095883e-08\n",
      "\n",
      "Epoch 940/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.297718734320952e-08\n",
      "test MSELoss: 6.713221516463363e-08\n",
      "\n",
      "Epoch 950/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.297575088367514e-08\n",
      "test MSELoss: 6.712891753579697e-08\n",
      "\n",
      "Epoch 960/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.297160032832869e-08\n",
      "test MSELoss: 6.713536393476716e-08\n",
      "\n",
      "Epoch 970/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.29776491171262e-08\n",
      "test MSELoss: 6.714158207188348e-08\n",
      "\n",
      "Epoch 980/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.297174594621567e-08\n",
      "test MSELoss: 6.714502021054614e-08\n",
      "\n",
      "Epoch 990/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.296806975503536e-08\n",
      "test MSELoss: 6.712756075444304e-08\n",
      "\n",
      "Epoch 1000/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.29683977757881e-08\n",
      "test MSELoss: 6.71208427505121e-08\n",
      "\n",
      "Epoch 1000/1000, Learning rate 1.0000000000000004e-08\n",
      "----------\n",
      "train MSELoss: 6.29683977757881e-08\n",
      "test MSELoss: 6.71208427505121e-08\n",
      "\n",
      "No early stopping: 980th training complete in 0h 12m 26s\n",
      "----------\n",
      "Best train MSELoss: 6.294418142260838e-08\n",
      "Best test MSELoss: 6.71038966615356e-08\n",
      "\n",
      "Saving after 1000th training to ./model/AE_v2_swish_gauss.tar\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSELoss of AE: 2.23305864e-04\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 184.00 MiB (GPU 0; 15.78 GiB total capacity; 1.29 GiB already allocated; 351.50 MiB free; 1.47 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bc8f0a85f840>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;31m# compute MSELoss (PyTorch model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolution_snapshot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m \u001b[0mtarget_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSELoss of AE: {:.8e}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/NM-ROM/nm-rom/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-760dd7b23540>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, T)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/NM-ROM/nm-rom/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/NM-ROM/nm-rom/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/NM-ROM/nm-rom/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-760dd7b23540>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msilu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-760dd7b23540>\u001b[0m in \u001b[0;36msilu\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msilu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSiLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 184.00 MiB (GPU 0; 15.78 GiB total capacity; 1.29 GiB already allocated; 351.50 MiB free; 1.47 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "\n",
    "# set checkpoint\n",
    "PATH = './checkpoint_v2.tar'\n",
    "\n",
    "\n",
    "# set the number of nodes in each layer\n",
    "m = 1000\n",
    "f = 4\n",
    "b = 36\n",
    "db = 12\n",
    "M2 = b + (m-1)*db\n",
    "M1 = 2*m\n",
    "\n",
    "\n",
    "# load model\n",
    "try:\n",
    "    checkpoint = torch.load(PATH, map_location=device)\n",
    "    \n",
    "    encoder = Encoder(m,M1,f).to(device)\n",
    "    decoder = Decoder(f,M2,m).to(device)\n",
    "    \n",
    "    # Prune\n",
    "    mask = create_mask_v2(m,b,db)\n",
    "    prune.custom_from_mask(decoder.full[2], name='weight', mask=torch.tensor(mask).to(device))    \n",
    "    \n",
    "#     optimizer = torch.optim.LBFGS(list(encoder.parameters()) + list(decoder.parameters()), lr=1)\n",
    "#     scheduler = None     \n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,patience=10) \n",
    "    \n",
    "    loss_func = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    last_epoch = checkpoint['epoch']\n",
    "    loss_hist = checkpoint['loss_hist']\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    early_stop_counter = checkpoint['early_stop_counter']\n",
    "    best_encoder_wts = checkpoint['best_encoder_wts']\n",
    "    best_decoder_wts = checkpoint['best_decoder_wts']\n",
    "    \n",
    "    print(\"\\n--------checkpoint restored--------\\n\")\n",
    "    \n",
    "    # compute sparsity in mask\n",
    "    mask = decoder.state_dict()['full.2.weight_mask']\n",
    "    print(\"Sparsity in {} by {} mask: {:.2f}%\".format(\n",
    "        mask.shape[0], mask.shape[1], 100. * float(torch.sum(mask == 0))/ float(mask.nelement())))\n",
    "\n",
    "    # resume training\n",
    "    print(\"\")\n",
    "    print('Re-start {}th training... m={}, f={}, M1={}, M2={}, b={}, db={}'.format(\n",
    "        last_epoch+1, m, f, M1, M2, b, db))\n",
    "except:\n",
    "    encoder = Encoder(m,M1,f).to(device)\n",
    "    decoder = Decoder(f,M2,m).to(device)\n",
    "    \n",
    "    # Prune\n",
    "    mask = create_mask_v2(m,b,db)\n",
    "    prune.custom_from_mask(decoder.full[2], name='weight', mask=torch.tensor(mask).to(device))\n",
    "\n",
    "#     optimizer = torch.optim.LBFGS(list(encoder.parameters()) + list(decoder.parameters()), lr=1)\n",
    "#     scheduler = None \n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,patience=10) \n",
    "    \n",
    "    loss_func = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    last_epoch = 0\n",
    "    loss_hist = {'train':[],'test':[]}\n",
    "    best_loss = float(\"inf\")\n",
    "    early_stop_counter = 1\n",
    "    best_encoder_wts = copy.deepcopy(encoder.state_dict())\n",
    "    best_decoder_wts = copy.deepcopy(decoder.state_dict())\n",
    "    \n",
    "    print(\"\\n--------checkpoint not restored--------\\n\")\n",
    "    \n",
    "    # compute sparsity in mask\n",
    "    mask = decoder.state_dict()['full.2.weight_mask']\n",
    "    print(\"Sparsity in {} by {} mask: {:.2f}%\".format(\n",
    "        mask.shape[0], mask.shape[1], 100. * float(torch.sum(mask == 0))/ float(mask.nelement())))\n",
    "\n",
    "    # start training\n",
    "    print(\"\")\n",
    "    print('Start first training... m={}, f={}, M1={}, M2={}, b={}, db={}'.format(\n",
    "        m, f, M1, M2, b, db))\n",
    "pass\n",
    "\n",
    "\n",
    "# train model\n",
    "since = time.time()\n",
    "\n",
    "for epoch in range(last_epoch+1,num_epochs+1):   \n",
    "\n",
    "    if epoch%num_epochs_print == 0:\n",
    "        print()\n",
    "        if scheduler !=None:\n",
    "            print('Epoch {}/{}, Learning rate {}'.format(\n",
    "                epoch, num_epochs, optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "        else:\n",
    "            print('Epoch {}/{}'.format(\n",
    "                epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and test phase\n",
    "    for phase in ['train', 'test']:\n",
    "        if phase == 'train':\n",
    "            encoder.train()  # Set model to training mode\n",
    "            decoder.train()  # Set model to training mode\n",
    "        else:\n",
    "            encoder.eval()   # Set model to evaluation mode\n",
    "            decoder.eval()   # Set model to evaluation mode\n",
    "            \n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Iterate over data\n",
    "        for data, in data_loaders[phase]:\n",
    "            inputs = data.to(device)\n",
    "            targets = data.to(device)\n",
    "\n",
    "            if phase == 'train':\n",
    "                if scheduler != None:\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    outputs = decoder(encoder(inputs))\n",
    "                    loss = loss_func(outputs, targets)\n",
    "\n",
    "                    # backward\n",
    "                    loss.backward()\n",
    "\n",
    "                    # optimize\n",
    "                    optimizer.step()  \n",
    "                    \n",
    "                    # add running loss\n",
    "                    running_loss += loss.item()*inputs.shape[0]\n",
    "                else:\n",
    "                    def closure():\n",
    "                        # zero the parameter gradients\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # forward\n",
    "                        outputs = decoder(encoder(inputs))\n",
    "                        loss = loss_func(outputs,targets)\n",
    "\n",
    "                        # backward\n",
    "                        loss.backward()\n",
    "                        return loss\n",
    "\n",
    "                    # optimize\n",
    "                    optimizer.step(closure)\n",
    "                    \n",
    "                    # add running loss\n",
    "                    with torch.set_grad_enabled(False):\n",
    "                        outputs = decoder(encoder(inputs))\n",
    "                        running_loss += loss_func(outputs,targets).item()*inputs.shape[0]\n",
    "                    \n",
    "            else:\n",
    "                with torch.set_grad_enabled(False):\n",
    "                    outputs = decoder(encoder(inputs))\n",
    "                    running_loss += loss_func(outputs,targets).item()*inputs.shape[0]\n",
    "\n",
    "        # compute epoch loss\n",
    "        epoch_loss = running_loss / dataset_shapes[phase][0]\n",
    "        loss_hist[phase].append(epoch_loss)\n",
    "            \n",
    "        # update learning rate\n",
    "        if phase == 'train' and scheduler != None:\n",
    "            scheduler.step(epoch_loss)\n",
    "\n",
    "        if epoch%num_epochs_print == 0:\n",
    "            print('{} MSELoss: {}'.format(\n",
    "                phase, epoch_loss))\n",
    "\n",
    "    # deep copy the model\n",
    "    if loss_hist['test'][-1] < best_loss:\n",
    "        best_loss = loss_hist['test'][-1]\n",
    "        early_stop_counter = 1\n",
    "        best_encoder_wts = copy.deepcopy(encoder.state_dict())\n",
    "        best_decoder_wts = copy.deepcopy(decoder.state_dict())\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= early_stop_patience:  \n",
    "            break\n",
    "    \n",
    "    # save checkpoint every num_epoch_print\n",
    "    if epoch%num_epochs_print== 0:\n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'encoder_state_dict': encoder.state_dict(),\n",
    "                    'decoder_state_dict': decoder.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss_hist': loss_hist,\n",
    "                    'best_loss': best_loss,\n",
    "                    'early_stop_counter': early_stop_counter,\n",
    "                    'best_encoder_wts': best_encoder_wts,\n",
    "                    'best_decoder_wts': best_decoder_wts,\n",
    "                    }, PATH)        \n",
    "\n",
    "print()\n",
    "print('Epoch {}/{}, Learning rate {}'      .format(epoch, num_epochs, optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "print('-' * 10)\n",
    "print('train MSELoss: {}'.format(loss_hist['train'][-1]))\n",
    "print('test MSELoss: {}'.format(loss_hist['test'][-1]))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "\n",
    "# load best model weights\n",
    "encoder.load_state_dict(best_encoder_wts)\n",
    "decoder.load_state_dict(best_decoder_wts)\n",
    "\n",
    "# compute best train MSELoss\n",
    "# encoder.to('cpu').eval()\n",
    "# decoder.to('cpu').eval()\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    train_inputs = torch.tensor(trainset).to(device)\n",
    "    train_targets = torch.tensor(trainset).to(device)\n",
    "    train_outputs = decoder(encoder(train_inputs))\n",
    "    train_loss = loss_func(train_outputs,train_targets).item()\n",
    "\n",
    "# print out training time and best results\n",
    "print()\n",
    "if epoch < num_epochs:\n",
    "    print('Early stopping: {}th training complete in {:.0f}h {:.0f}m {:.0f}s'          .format(epoch-last_epoch, time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "else:\n",
    "    print('No early stopping: {}th training complete in {:.0f}h {:.0f}m {:.0f}s'          .format(epoch-last_epoch, time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "print('-' * 10)\n",
    "print('Best train MSELoss: {}'.format(train_loss))\n",
    "print('Best test MSELoss: {}'.format(best_loss))\n",
    "\n",
    "###### save models ########\n",
    "print()\n",
    "print(\"Saving after {}th training to\".format(epoch),\n",
    "      \"./model/AE_v2_swish_gauss.tar\")\n",
    "torch.save({'encoder_state_dict': encoder.state_dict(), 'decoder_state_dict': decoder.state_dict()}, './model/AE_git.tar')\n",
    "\n",
    "\n",
    "# plot train and test loss\n",
    "plt.figure()\n",
    "plt.semilogy(loss_hist['train'])\n",
    "plt.semilogy(loss_hist['test'])\n",
    "plt.legend(['train','test'])\n",
    "plt.show()   \n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "# release gpu memory\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# load weights and bias\n",
    "en_w1=encoder.full[0].weight.cpu().detach().numpy().astype('float32')\n",
    "en_b1=encoder.full[0].bias.cpu().detach().numpy().astype('float32')\n",
    "en_w2=encoder.full[2].weight.cpu().detach().numpy().astype('float32')\n",
    "\n",
    "de_w1=decoder.full[0].weight.cpu().detach().numpy().astype('float32')\n",
    "de_w2=decoder.full[2].weight.cpu().detach().numpy().astype('float32')\n",
    "\n",
    "de_w2_sp=sp.csr_matrix(de_w2,dtype='float32')\n",
    "\n",
    "de_w1T=de_w1.T\n",
    "de_w2T=de_w2.T\n",
    "\n",
    "de_w2T_sp=de_w2_sp.T\n",
    "\n",
    "\n",
    "# numpy version of AE\n",
    "def sigmoid_np(input):\n",
    "    return (1.0/(1.0+np.exp(-input))).astype('float32')\n",
    "\n",
    "def encoder_np_forward(x):\n",
    "    z1 = en_w1.dot(x) + en_b1\n",
    "    s1 = sigmoid_np(z1)\n",
    "    a1 = z1*s1\n",
    "    y = en_w2.dot(a1)   \n",
    "    return y\n",
    "\n",
    "def decoder_np_forward(x):\n",
    "    z1 = de_w1.dot(x)\n",
    "    s1 = sigmoid_np(z1)\n",
    "    a1 = z1*s1\n",
    "    y = de_w2.dot(a1)  \n",
    "    return y\n",
    "\n",
    "def decoder_sp_forward(x):\n",
    "    z1 = de_w1.dot(x)\n",
    "    s1 = sigmoid_np(z1)\n",
    "    a1 = z1*s1\n",
    "    y = sp.csr_matrix.dot(de_w2_sp,a1)\n",
    "    return y\n",
    "\n",
    "def decoder_np_forward_backward(x):\n",
    "    z1 = de_w1.dot(x)\n",
    "    s1 = sigmoid_np(z1)\n",
    "    a1 = z1*s1\n",
    "    y = de_w2.dot(a1)\n",
    "\n",
    "    dout = de_w1T\n",
    "    dout = (s1 + a1*(1-s1))*dout\n",
    "    dydxT = dout.dot(de_w2T)   \n",
    "    return y,dydxT.T\n",
    "\n",
    "def decoder_sp_forward_backward(x):\n",
    "    z1 = de_w1.dot(x) \n",
    "    s1 = sigmoid_np(z1)\n",
    "    a1 = z1*s1\n",
    "    y = sp.csr_matrix.dot(de_w2_sp,a1)\n",
    "\n",
    "    dout = de_w1T\n",
    "    dout = (s1 + a1*(1-s1))*dout\n",
    "    dydxT = sp.csr_matrix.dot(dout,de_w2T_sp)\n",
    "    return y,dydxT.T\n",
    "\n",
    "\n",
    "# compute MSELoss (numpy version)\n",
    "comp_orig_data=np.zeros((nset*nt,f))\n",
    "rest_orig_data=np.zeros(solution_snapshot.shape)\n",
    "\n",
    "for k in range(nset*nt):\n",
    "    comp_orig_data[k]=encoder_np_forward(solution_snapshot[k])\n",
    "    rest_orig_data[k]=decoder_sp_forward(comp_orig_data[k])\n",
    "    \n",
    "print(\"MSELoss of AE: {:.8e}\".format(np.linalg.norm(solution_snapshot-rest_orig_data)**2/np.prod(solution_snapshot.shape)))\n",
    "\n",
    "# compute MSELoss (PyTorch model)\n",
    "input_data=torch.tensor(solution_snapshot).to(device)\n",
    "target_data=decoder(encoder(input_data))\n",
    "\n",
    "print(\"MSELoss of AE: {:.8e}\".format(torch.nn.functional.mse_loss(input_data,target_data).detach().item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete checkpoint\n",
    "try:\n",
    "    os.remove(PATH)\n",
    "    print()\n",
    "    print(\"checkpoint removed\")\n",
    "except:\n",
    "    print(\"no checkpoint exists\")\n",
    "    \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NM-ROM",
   "language": "python",
   "name": "nm-rom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
