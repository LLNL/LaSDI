{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cb332308a85e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manimation\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0manimation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# get_ipython().run_line_magic('matplotlib', 'inline')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/tce/packages/python/python-3.7.2/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend_bases\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigaspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridspec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcParamsDefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcParamsOrig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/tce/packages/python/python-3.7.2/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubplotBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_input\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlockingMouseInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlockingKeyMouseInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridspec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/tce/packages/python/python-3.7.2/lib/python3.7/site-packages/matplotlib/axes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_subplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_axes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/tce/packages/python/python-3.7.2/lib/python3.7/site-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martist\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmartist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridspec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSpec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubplotSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layoutbox\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlayoutbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/tce/packages/python/python-3.7.2/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticker\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmticker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtri\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmtri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m from matplotlib.cbook import (\n\u001b[1;32m     38\u001b[0m     \u001b[0mMatplotlibDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn_deprecated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTEP_LOOKUP_MAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/tce/packages/python/python-3.7.2/lib/python3.7/site-packages/matplotlib/tri/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrifinder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtriinterpolate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrirefine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtripcolor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtriplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/tce/packages/python/python-3.7.2/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/tce/packages/python/python-3.7.2/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/tce/packages/python/python-3.7.2/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/tce/packages/python/python-3.7.2/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/tce/packages/python/python-3.7.2/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m/usr/tce/packages/python/python-3.7.2/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "\n",
    "from scipy import sparse as sp\n",
    "from scipy import sparse\n",
    "from scipy.sparse import spdiags\n",
    "from scipy.sparse import linalg\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.io import savemat,loadmat\n",
    "import scipy.integrate as integrate\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation \n",
    "\n",
    "from itertools import combinations_with_replacement, product\n",
    "from sklearn.decomposition import SparseCoder\n",
    "from tqdm.notebook import tqdm,trange\n",
    "import sys,time\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import pysindy as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Auto-Encoder\n",
    "We now train the auto-encoder as per the \"train_NM-ROM_swish\" file. We use the snapshot data from above to train the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose device that is not being used\n",
    "gpu_ids = \"0\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=gpu_ids\n",
    "\n",
    "def create_mask_v2(m,b,db):\n",
    "    '''\n",
    "    mask=create_mask_v2(m,b,db)  \n",
    "    '''\n",
    "    \n",
    "    M2 = b + db*(m-1)\n",
    "    mask = np.zeros((m,M2),dtype='int')\n",
    "    \n",
    "    block = np.ones(b,dtype='int')\n",
    "    ind = np.arange(b)\n",
    "    for row in range(m):\n",
    "        col = ind + row*db\n",
    "        mask[row,col] = block\n",
    "    \n",
    "    print(\n",
    "        \"Sparsity in {} by {} mask: {:.2f}%\".format(\n",
    "            m, M2, (1.0-np.count_nonzero(mask)/mask.size)*100\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.spy(mask)\n",
    "    plt.show()\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def silu(input):\n",
    "    return input * torch.sigmoid(input)\n",
    "class SiLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return silu(input)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,m,M1,f):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.full = nn.Sequential(\n",
    "            nn.Linear(m,M1),\n",
    "            SiLU(),\n",
    "            nn.Linear(M1,f,bias=False)\n",
    "        )\n",
    "                \n",
    "    def forward(self, y):     \n",
    "        y = y.view(-1,m)\n",
    "        T = self.full(y)\n",
    "        T = T.squeeze()\n",
    "        \n",
    "        return T\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,f,M2,m):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.full = nn.Sequential(\n",
    "            nn.Linear(f,M2,bias=False),\n",
    "            SiLU(),\n",
    "            nn.Linear(M2,m,bias=False)\n",
    "        )\n",
    "             \n",
    "    def forward(self,T):\n",
    "        T = T.view(-1,f)\n",
    "        y = self.full(T)\n",
    "        y = y.squeeze()\n",
    "        \n",
    "        return y\n",
    "\n",
    "# load solution snapshot\n",
    "solution_snapshot_orig = snapshot_full\n",
    "\n",
    "# substract IC -> centered on IC\n",
    "ndata=solution_snapshot_orig.shape[0]\n",
    "nset= round(ndata/(nt+1))\n",
    "\n",
    "solution_snapshot=np.array([])\n",
    "for foo in range(nset):\n",
    "    solution_snapshot=np.append(solution_snapshot,solution_snapshot_orig[foo*(nt+1):(foo+1)*(nt+1)])#    -solution_snapshot_orig[foo*(nt+1)])\n",
    "\n",
    "solution_snapshot=np.reshape(solution_snapshot,(-1,nx))\n",
    "\n",
    "# remove BC\n",
    "solution_snapshot = solution_snapshot[:,:-1].astype('float32')\n",
    "\n",
    "# define testset and trainset indices\n",
    "test_ind = np.random.permutation(np.arange(solution_snapshot.shape[0]))[:int(0.1*solution_snapshot.shape[0])]\n",
    "train_ind = np.setdiff1d(np.arange(solution_snapshot.shape[0]),test_ind)\n",
    "\n",
    "# set trainset and testset\n",
    "trainset = solution_snapshot[train_ind]\n",
    "testset = solution_snapshot[test_ind] \n",
    "\n",
    "# set dataset\n",
    "dataset = {'train':data_utils.TensorDataset(torch.tensor(trainset)),\n",
    "           'test':data_utils.TensorDataset(torch.tensor(testset))}\n",
    "print(dataset['train'].tensors[0].shape, dataset['test'].tensors[0].shape)\n",
    "\n",
    "\n",
    "# compute dataset shapes\n",
    "dataset_shapes = {'train':trainset.shape,\n",
    "                 'test':testset.shape}\n",
    "\n",
    "print(dataset_shapes['train'],dataset_shapes['test'])\n",
    "\n",
    "\n",
    "# set batch_size, number of epochs, paitience for early stop\n",
    "batch_size = 20\n",
    "num_epochs = 100\n",
    "num_epochs_print = num_epochs//100\n",
    "early_stop_patience = num_epochs//10\n",
    "\n",
    "\n",
    "# set data loaders\n",
    "train_loader = DataLoader(dataset=dataset['train'], \n",
    "                          batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(dataset=dataset['test'], \n",
    "                         batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "data_loaders = {'train':train_loader, 'test':test_loader}\n",
    "\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device, '\\n')\n",
    "\n",
    "\n",
    "# set checkpoint\n",
    "PATH = './checkpoint_v2.tar'\n",
    "\n",
    "\n",
    "# set the number of nodes in each layer\n",
    "m = 1000\n",
    "f = 5\n",
    "b = 36\n",
    "db = 12\n",
    "M2 = b + (m-1)*db\n",
    "M1 = 2*m\n",
    "\n",
    "\n",
    "# load model\n",
    "try:\n",
    "    checkpoint = torch.load(PATH, map_location=device)\n",
    "    \n",
    "    encoder = Encoder(m,M1,f).to(device)\n",
    "    decoder = Decoder(f,M2,m).to(device)\n",
    "    \n",
    "    # Prune\n",
    "    mask = create_mask_v2(m,b,db)\n",
    "    prune.custom_from_mask(decoder.full[2], name='weight', mask=torch.tensor(mask).to(device))    \n",
    "    \n",
    "#     optimizer = torch.optim.LBFGS(list(encoder.parameters()) + list(decoder.parameters()), lr=1)\n",
    "#     scheduler = None     \n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,patience=10) \n",
    "    \n",
    "    loss_func = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    last_epoch = checkpoint['epoch']\n",
    "    loss_hist = checkpoint['loss_hist']\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    early_stop_counter = checkpoint['early_stop_counter']\n",
    "    best_encoder_wts = checkpoint['best_encoder_wts']\n",
    "    best_decoder_wts = checkpoint['best_decoder_wts']\n",
    "    \n",
    "    print(\"\\n--------checkpoint restored--------\\n\")\n",
    "    \n",
    "    # compute sparsity in mask\n",
    "    mask = decoder.state_dict()['full.2.weight_mask']\n",
    "    print(\"Sparsity in {} by {} mask: {:.2f}%\".format(\n",
    "        mask.shape[0], mask.shape[1], 100. * float(torch.sum(mask == 0))/ float(mask.nelement())))\n",
    "\n",
    "    # resume training\n",
    "    print(\"\")\n",
    "    print('Re-start {}th training... m={}, f={}, M1={}, M2={}, b={}, db={}'.format(\n",
    "        last_epoch+1, m, f, M1, M2, b, db))\n",
    "except:\n",
    "    encoder = Encoder(m,M1,f).to(device)\n",
    "    decoder = Decoder(f,M2,m).to(device)\n",
    "    \n",
    "    # Prune\n",
    "    mask = create_mask_v2(m,b,db)\n",
    "    prune.custom_from_mask(decoder.full[2], name='weight', mask=torch.tensor(mask).to(device))\n",
    "\n",
    "#     optimizer = torch.optim.LBFGS(list(encoder.parameters()) + list(decoder.parameters()), lr=1)\n",
    "#     scheduler = None \n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,patience=10) \n",
    "    \n",
    "    loss_func = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    last_epoch = 0\n",
    "    loss_hist = {'train':[],'test':[]}\n",
    "    best_loss = float(\"inf\")\n",
    "    early_stop_counter = 1\n",
    "    best_encoder_wts = copy.deepcopy(encoder.state_dict())\n",
    "    best_decoder_wts = copy.deepcopy(decoder.state_dict())\n",
    "    \n",
    "    print(\"\\n--------checkpoint not restored--------\\n\")\n",
    "    \n",
    "    # compute sparsity in mask\n",
    "    mask = decoder.state_dict()['full.2.weight_mask']\n",
    "    print(\"Sparsity in {} by {} mask: {:.2f}%\".format(\n",
    "        mask.shape[0], mask.shape[1], 100. * float(torch.sum(mask == 0))/ float(mask.nelement())))\n",
    "\n",
    "    # start training\n",
    "    print(\"\")\n",
    "    print('Start first training... m={}, f={}, M1={}, M2={}, b={}, db={}'.format(\n",
    "        m, f, M1, M2, b, db))\n",
    "pass\n",
    "\n",
    "\n",
    "# train model\n",
    "since = time.time()\n",
    "\n",
    "for epoch in range(last_epoch+1,num_epochs+1):   \n",
    "\n",
    "    if epoch%num_epochs_print == 0:\n",
    "        print()\n",
    "        if scheduler !=None:\n",
    "            print('Epoch {}/{}, Learning rate {}'.format(\n",
    "                epoch, num_epochs, optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "        else:\n",
    "            print('Epoch {}/{}'.format(\n",
    "                epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and test phase\n",
    "    for phase in ['train', 'test']:\n",
    "        if phase == 'train':\n",
    "            encoder.train()  # Set model to training mode\n",
    "            decoder.train()  # Set model to training mode\n",
    "        else:\n",
    "            encoder.eval()   # Set model to evaluation mode\n",
    "            decoder.eval()   # Set model to evaluation mode\n",
    "            \n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Iterate over data\n",
    "        for data, in data_loaders[phase]:\n",
    "            inputs = data.to(device)\n",
    "            targets = data.to(device)\n",
    "\n",
    "            if phase == 'train':\n",
    "                if scheduler != None:\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    outputs = decoder(encoder(inputs))\n",
    "                    loss = loss_func(outputs, targets)\n",
    "\n",
    "                    # backward\n",
    "                    loss.backward()\n",
    "\n",
    "                    # optimize\n",
    "                    optimizer.step()  \n",
    "                    \n",
    "                    # add running loss\n",
    "                    running_loss += loss.item()*inputs.shape[0]\n",
    "                else:\n",
    "                    def closure():\n",
    "                        # zero the parameter gradients\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # forward\n",
    "                        outputs = decoder(encoder(inputs))\n",
    "                        loss = loss_func(outputs,targets)\n",
    "\n",
    "                        # backward\n",
    "                        loss.backward()\n",
    "                        return loss\n",
    "\n",
    "                    # optimize\n",
    "                    optimizer.step(closure)\n",
    "                    \n",
    "                    # add running loss\n",
    "                    with torch.set_grad_enabled(False):\n",
    "                        outputs = decoder(encoder(inputs))\n",
    "                        running_loss += loss_func(outputs,targets).item()*inputs.shape[0]\n",
    "                    \n",
    "            else:\n",
    "                with torch.set_grad_enabled(False):\n",
    "                    outputs = decoder(encoder(inputs))\n",
    "                    running_loss += loss_func(outputs,targets).item()*inputs.shape[0]\n",
    "\n",
    "        # compute epoch loss\n",
    "        epoch_loss = running_loss / dataset_shapes[phase][0]\n",
    "        loss_hist[phase].append(epoch_loss)\n",
    "            \n",
    "        # update learning rate\n",
    "        if phase == 'train' and scheduler != None:\n",
    "            scheduler.step(epoch_loss)\n",
    "\n",
    "        if epoch%num_epochs_print == 0:\n",
    "            print('{} MSELoss: {}'.format(\n",
    "                phase, epoch_loss))\n",
    "\n",
    "    # deep copy the model\n",
    "    if loss_hist['test'][-1] < best_loss:\n",
    "        best_loss = loss_hist['test'][-1]\n",
    "        early_stop_counter = 1\n",
    "        best_encoder_wts = copy.deepcopy(encoder.state_dict())\n",
    "        best_decoder_wts = copy.deepcopy(decoder.state_dict())\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= early_stop_patience:  \n",
    "            break\n",
    "    \n",
    "    # save checkpoint every num_epoch_print\n",
    "    if epoch%num_epochs_print== 0:\n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'encoder_state_dict': encoder.state_dict(),\n",
    "                    'decoder_state_dict': decoder.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss_hist': loss_hist,\n",
    "                    'best_loss': best_loss,\n",
    "                    'early_stop_counter': early_stop_counter,\n",
    "                    'best_encoder_wts': best_encoder_wts,\n",
    "                    'best_decoder_wts': best_decoder_wts,\n",
    "                    }, PATH)        \n",
    "\n",
    "print()\n",
    "print('Epoch {}/{}, Learning rate {}'      .format(epoch, num_epochs, optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "print('-' * 10)\n",
    "print('train MSELoss: {}'.format(loss_hist['train'][-1]))\n",
    "print('test MSELoss: {}'.format(loss_hist['test'][-1]))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "\n",
    "# load best model weights\n",
    "encoder.load_state_dict(best_encoder_wts)\n",
    "decoder.load_state_dict(best_decoder_wts)\n",
    "\n",
    "# compute best train MSELoss\n",
    "# encoder.to('cpu').eval()\n",
    "# decoder.to('cpu').eval()\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    train_inputs = torch.tensor(trainset).to(device)\n",
    "    train_targets = torch.tensor(trainset).to(device)\n",
    "    train_outputs = decoder(encoder(train_inputs))\n",
    "    train_loss = loss_func(train_outputs,train_targets).item()\n",
    "\n",
    "# print out training time and best results\n",
    "print()\n",
    "if epoch < num_epochs:\n",
    "    print('Early stopping: {}th training complete in {:.0f}h {:.0f}m {:.0f}s'          .format(epoch-last_epoch, time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "else:\n",
    "    print('No early stopping: {}th training complete in {:.0f}h {:.0f}m {:.0f}s'          .format(epoch-last_epoch, time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "print('-' * 10)\n",
    "print('Best train MSELoss: {}'.format(train_loss))\n",
    "print('Best test MSELoss: {}'.format(best_loss))\n",
    "\n",
    "###### save models ########\n",
    "print()\n",
    "print(\"Saving after {}th training to\".format(epoch),\n",
    "      \"./model/AE_v2_swish_gauss.tar\")\n",
    "torch.save({'encoder_state_dict': encoder.state_dict(), 'decoder_state_dict': decoder.state_dict()}, './model/AE_guass.tar')\n",
    "\n",
    "\n",
    "# plot train and test loss\n",
    "plt.figure()\n",
    "plt.semilogy(loss_hist['train'])\n",
    "plt.semilogy(loss_hist['test'])\n",
    "plt.legend(['train','test'])\n",
    "plt.show()   \n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "# release gpu memory\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# load weights and bias\n",
    "en_w1=encoder.full[0].weight.cpu().detach().numpy().astype('float32')\n",
    "en_b1=encoder.full[0].bias.cpu().detach().numpy().astype('float32')\n",
    "en_w2=encoder.full[2].weight.cpu().detach().numpy().astype('float32')\n",
    "\n",
    "de_w1=decoder.full[0].weight.cpu().detach().numpy().astype('float32')\n",
    "de_w2=decoder.full[2].weight.cpu().detach().numpy().astype('float32')\n",
    "\n",
    "de_w2_sp=sp.csr_matrix(de_w2,dtype='float32')\n",
    "\n",
    "de_w1T=de_w1.T\n",
    "de_w2T=de_w2.T\n",
    "\n",
    "de_w2T_sp=de_w2_sp.T\n",
    "\n",
    "\n",
    "# numpy version of AE\n",
    "def sigmoid_np(input):\n",
    "    return (1.0/(1.0+np.exp(-input))).astype('float32')\n",
    "\n",
    "def encoder_np_forward(x):\n",
    "    z1 = en_w1.dot(x) + en_b1\n",
    "    s1 = sigmoid_np(z1)\n",
    "    a1 = z1*s1\n",
    "    y = en_w2.dot(a1)   \n",
    "    return y\n",
    "\n",
    "def decoder_np_forward(x):\n",
    "    z1 = de_w1.dot(x)\n",
    "    s1 = sigmoid_np(z1)\n",
    "    a1 = z1*s1\n",
    "    y = de_w2.dot(a1)  \n",
    "    return y\n",
    "\n",
    "def decoder_sp_forward(x):\n",
    "    z1 = de_w1.dot(x)\n",
    "    s1 = sigmoid_np(z1)\n",
    "    a1 = z1*s1\n",
    "    y = sp.csr_matrix.dot(de_w2_sp,a1)\n",
    "    return y\n",
    "\n",
    "def decoder_np_forward_backward(x):\n",
    "    z1 = de_w1.dot(x)\n",
    "    s1 = sigmoid_np(z1)\n",
    "    a1 = z1*s1\n",
    "    y = de_w2.dot(a1)\n",
    "\n",
    "    dout = de_w1T\n",
    "    dout = (s1 + a1*(1-s1))*dout\n",
    "    dydxT = dout.dot(de_w2T)   \n",
    "    return y,dydxT.T\n",
    "\n",
    "def decoder_sp_forward_backward(x):\n",
    "    z1 = de_w1.dot(x) \n",
    "    s1 = sigmoid_np(z1)\n",
    "    a1 = z1*s1\n",
    "    y = sp.csr_matrix.dot(de_w2_sp,a1)\n",
    "\n",
    "    dout = de_w1T\n",
    "    dout = (s1 + a1*(1-s1))*dout\n",
    "    dydxT = sp.csr_matrix.dot(dout,de_w2T_sp)\n",
    "    return y,dydxT.T\n",
    "\n",
    "\n",
    "# compute MSELoss (numpy version)\n",
    "comp_orig_data=np.zeros((nset*nt,f))\n",
    "rest_orig_data=np.zeros(solution_snapshot.shape)\n",
    "\n",
    "for k in range(nset*nt):\n",
    "    comp_orig_data[k]=encoder_np_forward(solution_snapshot[k])\n",
    "    rest_orig_data[k]=decoder_sp_forward(comp_orig_data[k])\n",
    "    \n",
    "print(\"MSELoss of AE: {:.8e}\".format(np.linalg.norm(solution_snapshot-rest_orig_data)**2/np.prod(solution_snapshot.shape)))\n",
    "\n",
    "# compute MSELoss (PyTorch model)\n",
    "input_data=torch.tensor(solution_snapshot).to(device)\n",
    "target_data=decoder(encoder(input_data))\n",
    "\n",
    "print(\"MSELoss of AE: {:.8e}\".format(torch.nn.functional.mse_loss(input_data,target_data).detach().item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete checkpoint\n",
    "try:\n",
    "    os.remove(PATH)\n",
    "    print()\n",
    "    print(\"checkpoint removed\")\n",
    "except:\n",
    "    print(\"no checkpoint exists\")\n",
    "    \n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NM-ROM",
   "language": "python",
   "name": "nm-rom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
